"""
Development Assistant Agent - WebDancer-based agent for development workflows
"""

import asyncio
import json
from typing import Dict, List, Optional, Any
from datetime import datetime

from .base import WebDancerBasedAgent


class DevelopmentAssistantAgent(WebDancerBasedAgent):
    """
    Development Assistant Agent using WebDancer's multi-turn reasoning
    
    Specialized for:
    - Test completion and generation
    - Code analysis and bug detection
    - Implementation suggestions
    - Development workflow optimization
    - Code quality improvements
    """
    
    def __init__(self, tools: List[Any], config: Any):
        super().__init__(tools, config, "dev_assistant")
        self.development_domains = [
            "test_completion",
            "code_analysis", 
            "bug_detection",
            "implementation_suggestions",
            "code_quality",
            "development_workflow"
        ]
    
    async def analyze_and_complete_tests(self, file_path: str, context: Dict[str, Any] = None) -> Dict[str, Any]:
        """
        Analyze test file and provide completion suggestions
        
        Args:
            file_path: Path to test file to analyze
            context: Additional context for analysis
            
        Returns:
            Comprehensive analysis with completion suggestions
        """
        await self._rate_limit_check()
        
        # Check cache first
        cache_key = self._get_cache_key(f"analyze_tests_{file_path}", context or {})
        cached_result = self._get_cached_result(cache_key)
        if cached_result:
            return cached_result
        
        try:
            # Perform multi-turn reasoning for test analysis
            reasoning_result = await self._multi_turn_reasoning(
                f"Analyze and complete tests in {file_path}", 
                {"file_path": file_path, "context": context or {}}
            )
            
            # Execute test analysis
            analysis_result = await self._execute_test_analysis(
                file_path, context or {}, reasoning_result
            )
            
            # Generate completion suggestions
            completion_suggestions = await self._generate_completion_suggestions(
                file_path, analysis_result, reasoning_result
            )
            
            # Create comprehensive report
            final_result = await self._generate_development_report(
                file_path, analysis_result, completion_suggestions, reasoning_result
            )
            
            # Cache result
            self._set_cached_result(cache_key, final_result)
            
            return final_result
            
        except Exception as e:
            self.logger.error(f"Test analysis failed for {file_path}: {e}")
            return {
                "success": False,
                "file_path": file_path,
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }
    
    async def _execute_test_analysis(
        self, 
        file_path: str, 
        context: Dict[str, Any],
        reasoning_result: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Execute comprehensive test analysis"""
        
        analysis_result = {
            "file_path": file_path,
            "analysis_type": "test_completion",
            "incomplete_functions": [],
            "missing_implementations": [],
            "code_quality_issues": [],
            "security_gaps": [],
            "test_coverage_analysis": {}
        }
        
        # Use development assistant tool
        dev_tool = next((tool for tool in self.tools if tool.name == "dev_assistant"), None)
        if dev_tool:
            try:
                tool_analysis = await dev_tool.analyze_test_file(file_path)
                analysis_result.update(tool_analysis)
                
                # If it's payment tests, get payment-specific analysis
                if "payment" in file_path.lower():
                    payment_analysis = await dev_tool.analyze_payment_tests()
                    analysis_result["payment_specific"] = payment_analysis
                    
            except Exception as e:
                analysis_result["tool_error"] = str(e)
        
        return analysis_result
    
    async def _generate_completion_suggestions(
        self, 
        file_path: str, 
        analysis_result: Dict[str, Any],
        reasoning_result: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Generate intelligent completion suggestions"""
        
        suggestions = {
            "priority_completions": [],
            "implementation_templates": {},
            "best_practices": [],
            "security_improvements": [],
            "performance_optimizations": []
        }
        
        # Prioritize incomplete tests
        incomplete_tests = analysis_result.get("incomplete_tests", [])
        for test in incomplete_tests:
            priority = self._calculate_completion_priority(test)
            suggestions["priority_completions"].append({
                **test,
                "priority": priority,
                "estimated_effort": self._estimate_completion_effort(test)
            })
        
        # Sort by priority
        suggestions["priority_completions"].sort(
            key=lambda x: x["priority"], reverse=True
        )
        
        # Generate implementation templates
        for test in incomplete_tests[:5]:  # Top 5 priority tests
            function_name = test["function_name"]
            template = await self._generate_implementation_template(test, analysis_result)
            suggestions["implementation_templates"][function_name] = template
        
        # Best practices suggestions
        suggestions["best_practices"] = self._generate_best_practices(analysis_result)
        
        # Security improvements
        suggestions["security_improvements"] = self._generate_security_improvements(analysis_result)
        
        return suggestions
    
    def _calculate_completion_priority(self, test: Dict[str, Any]) -> int:
        """Calculate priority score for test completion"""
        priority = 0
        
        function_name = test.get("function_name", "").lower()
        docstring = test.get("docstring", "").lower()
        
        # High priority for security tests
        if any(keyword in function_name for keyword in ["security", "audit", "validation"]):
            priority += 50
        
        # High priority for financial operations
        if any(keyword in function_name for keyword in ["payment", "transaction", "money"]):
            priority += 40
        
        # Medium priority for error handling
        if any(keyword in function_name for keyword in ["error", "exception", "failure"]):
            priority += 30
        
        # Medium priority for rate limiting
        if "rate_limiting" in function_name:
            priority += 35
        
        # Lower priority for general tests
        if "test_" in function_name:
            priority += 10
        
        return priority
    
    def _estimate_completion_effort(self, test: Dict[str, Any]) -> str:
        """Estimate effort required to complete test"""
        function_name = test.get("function_name", "").lower()
        docstring = test.get("docstring", "")
        
        # Complex tests require more effort
        if any(keyword in function_name for keyword in ["integration", "end_to_end", "performance"]):
            return "high"
        elif any(keyword in function_name for keyword in ["security", "audit", "compliance"]):
            return "medium"
        else:
            return "low"
    
    async def _generate_implementation_template(
        self, 
        test: Dict[str, Any], 
        analysis_result: Dict[str, Any]
    ) -> str:
        """Generate implementation template for specific test"""
        
        function_name = test["function_name"]
        
        # Use development assistant tool to generate implementation
        dev_tool = next((tool for tool in self.tools if tool.name == "dev_assistant"), None)
        if dev_tool:
            try:
                implementation = await dev_tool.generate_test_completion(
                    analysis_result["file_path"], 
                    function_name
                )
                return implementation
            except Exception as e:
                self.logger.error(f"Failed to generate implementation for {function_name}: {e}")
        
        # Fallback generic template
        return f'''
def {function_name}(self):
    """
    {test.get("docstring", f"Implementation for {function_name}")}
    """
    # Arrange
    # TODO: Set up test data and dependencies
    
    # Act
    # TODO: Execute the functionality being tested
    
    # Assert
    # TODO: Verify expected behavior
    assert True  # Replace with actual assertions
        '''
    
    def _generate_best_practices(self, analysis_result: Dict[str, Any]) -> List[str]:
        """Generate best practices suggestions"""
        practices = []
        
        # Check for missing patterns
        if not analysis_result.get("incomplete_tests"):
            practices.append("âœ… All test functions are implemented")
        else:
            practices.append("ðŸ”§ Complete incomplete test functions for better coverage")
        
        practices.extend([
            "ðŸ§ª Use pytest fixtures for better test organization",
            "ðŸŽ¯ Use pytest.mark.parametrize for testing multiple scenarios",
            "ðŸ”’ Add security tests for all user inputs",
            "ðŸ“Š Include performance tests for critical operations",
            "ðŸ” Mock external dependencies to isolate unit tests",
            "ðŸ“ Write descriptive test names and docstrings",
            "âš¡ Use async/await properly for async operations",
            "ðŸ›¡ï¸ Test error conditions and edge cases thoroughly"
        ])
        
        return practices
    
    def _generate_security_improvements(self, analysis_result: Dict[str, Any]) -> List[str]:
        """Generate security improvement suggestions"""
        improvements = []
        
        # Check for payment-specific security
        if analysis_result.get("payment_specific"):
            improvements.extend([
                "ðŸ” Add tests for transaction replay attack prevention",
                "ðŸ’° Test for amount manipulation attempts",
                "ðŸ”‘ Verify proper authentication on payment endpoints",
                "â±ï¸ Test rate limiting on financial operations",
                "ðŸ“‹ Verify comprehensive audit logging",
                "ðŸ›¡ï¸ Test input sanitization for financial data",
                "ðŸ”’ Verify encryption of sensitive payment data"
            ])
        
        improvements.extend([
            "ðŸš« Test SQL injection prevention",
            "ðŸŒ Test XSS attack prevention", 
            "ðŸ“ Test path traversal prevention",
            "ðŸ” Test authentication bypass attempts",
            "âš¡ Test for timing attack vulnerabilities",
            "ðŸ” Verify proper error message handling (no info leakage)"
        ])
        
        return improvements
    
    async def _generate_development_report(
        self,
        file_path: str,
        analysis_result: Dict[str, Any],
        completion_suggestions: Dict[str, Any],
        reasoning_result: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Generate comprehensive development report"""
        
        report = {
            "success": True,
            "timestamp": datetime.now().isoformat(),
            "file_analysis": {
                "file_path": file_path,
                "total_functions": len(analysis_result.get("incomplete_tests", [])),
                "incomplete_functions": len([t for t in analysis_result.get("incomplete_tests", []) if t]),
                "completion_percentage": self._calculate_completion_percentage(analysis_result)
            },
            "priority_tasks": completion_suggestions.get("priority_completions", [])[:5],
            "implementation_templates": completion_suggestions.get("implementation_templates", {}),
            "recommendations": {
                "best_practices": completion_suggestions.get("best_practices", []),
                "security_improvements": completion_suggestions.get("security_improvements", []),
                "next_steps": self._generate_next_steps(analysis_result, completion_suggestions)
            },
            "reasoning_summary": {
                "total_turns": reasoning_result.get("total_turns", 0),
                "key_insights": self._extract_development_insights(reasoning_result)
            },
            "detailed_analysis": analysis_result
        }
        
        return report
    
    def _calculate_completion_percentage(self, analysis_result: Dict[str, Any]) -> float:
        """Calculate test completion percentage"""
        incomplete_tests = analysis_result.get("incomplete_tests", [])
        total_tests = len(incomplete_tests) + 10  # Assume some completed tests exist
        
        if total_tests == 0:
            return 100.0
        
        completed_tests = total_tests - len(incomplete_tests)
        return (completed_tests / total_tests) * 100
    
    def _generate_next_steps(
        self, 
        analysis_result: Dict[str, Any], 
        completion_suggestions: Dict[str, Any]
    ) -> List[str]:
        """Generate next steps for development"""
        steps = []
        
        priority_tasks = completion_suggestions.get("priority_completions", [])
        
        if priority_tasks:
            # Focus on highest priority incomplete tests
            top_task = priority_tasks[0]
            steps.append(f"ðŸŽ¯ Complete '{top_task['function_name']}' (Priority: {top_task['priority']})")
            
            if len(priority_tasks) > 1:
                steps.append(f"ðŸ“‹ Complete remaining {len(priority_tasks)-1} incomplete test functions")
        
        # Add general improvement steps
        steps.extend([
            "ðŸ§ª Run existing tests to ensure they pass",
            "ðŸ“Š Check test coverage with pytest-cov",
            "ðŸ”’ Review security test implementations",
            "âš¡ Add performance benchmarks for critical functions",
            "ðŸ“ Update documentation based on test implementations"
        ])
        
        return steps
    
    def _extract_development_insights(self, reasoning_result: Dict[str, Any]) -> List[str]:
        """Extract key insights from reasoning process"""
        insights = []
        
        reasoning_history = reasoning_result.get("reasoning_history", [])
        for step in reasoning_history:
            if step.get("conclusion"):
                insights.append(step["conclusion"])
        
        if not insights:
            insights = [
                "Multi-turn reasoning applied to test analysis",
                "Comprehensive code quality assessment completed",
                "Security and performance considerations evaluated"
            ]
        
        return insights
    
    async def complete_specific_test(
        self, 
        file_path: str, 
        function_name: str, 
        context: Dict[str, Any] = None
    ) -> Dict[str, Any]:
        """Complete a specific test function"""
        
        try:
            # Use development assistant tool
            dev_tool = next((tool for tool in self.tools if tool.name == "dev_assistant"), None)
            if not dev_tool:
                return {"error": "Development assistant tool not available"}
            
            implementation = await dev_tool.generate_test_completion(file_path, function_name)
            
            return {
                "success": True,
                "function_name": function_name,
                "implementation": implementation,
                "file_path": file_path,
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            return {
                "success": False,
                "function_name": function_name,
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }
    
    async def process_query(self, query: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """Implementation of abstract method from base class"""
        
        if "analyze test" in query.lower() or "payment test" in query.lower():
            file_path = context.get("file_path", "examples/tests/payment_tests.py")
            return await self.analyze_and_complete_tests(file_path, context)
        
        elif "complete test" in query.lower():
            file_path = context.get("file_path", "examples/tests/payment_tests.py")
            function_name = context.get("function_name", "")
            return await self.complete_specific_test(file_path, function_name, context)
        
        else:
            # General development assistance
            return await self.analyze_and_complete_tests(
                "examples/tests/payment_tests.py", 
                context
            )
